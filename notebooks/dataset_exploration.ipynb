{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ConvFinQA Dataset Analysis\n",
        "\n",
        "This notebook analyses the ConvFinQA dataset structure and content to understand:\n",
        "- Available dataset splits (train/dev/test)\n",
        "- Dataset statistics and characteristics  \n",
        "- Sample conversations and data quality\n",
        "- Evaluation recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('src')\n",
        "pd.set_option('display.max_columns', None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully\n",
            "Available splits: ['train', 'dev']\n",
            "Total conversations: 3458\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "with open('../data/convfinqa_dataset.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "print(\"Dataset loaded successfully\")\n",
        "print(f\"Available splits: {list(dataset.keys())}\")\n",
        "\n",
        "# Get basic counts\n",
        "total_conversations = sum(len(conversations) for conversations in dataset.values())\n",
        "print(f\"Total conversations: {total_conversations}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Split Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TRAIN split:\n",
            "  Conversations: 3037\n",
            "  Total turns: 11104\n",
            "  Average turns per conversation: 3.66\n",
            "\n",
            "DEV split:\n",
            "  Conversations: 421\n",
            "  Total turns: 1490\n",
            "  Average turns per conversation: 3.54\n",
            "\n",
            "Test set available: False\n",
            "Note: Test set not available. Use dev set for evaluation.\n"
          ]
        }
      ],
      "source": [
        "# Analyze each split\n",
        "split_stats = {}\n",
        "\n",
        "for split_name, conversations in dataset.items():\n",
        "    num_conversations = len(conversations)\n",
        "    \n",
        "    # Count total turns\n",
        "    total_turns = sum(len(conv['dialogue']['conv_questions']) for conv in conversations)\n",
        "    avg_turns = total_turns / num_conversations if num_conversations > 0 else 0\n",
        "    \n",
        "    split_stats[split_name] = {\n",
        "        'conversations': num_conversations,\n",
        "        'turns': total_turns,\n",
        "        'avg_turns': avg_turns\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{split_name.upper()} split:\")\n",
        "    print(f\"  Conversations: {num_conversations}\")\n",
        "    print(f\"  Total turns: {total_turns}\")\n",
        "    print(f\"  Average turns per conversation: {avg_turns:.2f}\")\n",
        "\n",
        "# Check test set availability\n",
        "test_available = 'test' in dataset\n",
        "print(f\"\\nTest set available: {test_available}\")\n",
        "if not test_available:\n",
        "    print(\"Note: Test set not available. Use dev set for evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Split Summary:\n",
            "       conversations  turns  avg_turns\n",
            "train           3037  11104       3.66\n",
            "dev              421   1490       3.54\n",
            "\n",
            "Paper statistics:\n",
            "  Train: 3,037 conversations\n",
            "  Dev: 421 conversations\n",
            "  Test: 434 conversations\n",
            "\n",
            "Your dataset matches paper splits: True\n"
          ]
        }
      ],
      "source": [
        "# Create summary table\n",
        "df = pd.DataFrame.from_dict(split_stats, orient='index')\n",
        "print(\"\\nSplit Summary:\")\n",
        "print(df.round(2))\n",
        "\n",
        "# Compare with paper statistics\n",
        "print(\"\\nPaper statistics:\")\n",
        "print(\"  Train: 3,037 conversations\")\n",
        "print(\"  Dev: 421 conversations\") \n",
        "print(\"  Test: 434 conversations\")\n",
        "\n",
        "print(\"\\nYour dataset matches paper splits:\", \n",
        "      split_stats.get('train', {}).get('conversations') == 3037 and \n",
        "      split_stats.get('dev', {}).get('conversations') == 421)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conversation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation length statistics:\n",
            "  Average: 3.64 turns\n",
            "  Median: 4 turns\n",
            "  Min: 1 turns\n",
            "  Max: 9 turns\n",
            "\n",
            "Conversation types:\n",
            "  Simple (Type I): 2448 (70.8%)\n",
            "  Hybrid (Type II): 1010 (29.2%)\n",
            "\n",
            "Length distribution:\n",
            "  1 turns: 6 conversations (0.2%)\n",
            "  2 turns: 858 conversations (24.8%)\n",
            "  3 turns: 751 conversations (21.7%)\n",
            "  4 turns: 940 conversations (27.2%)\n",
            "  5 turns: 649 conversations (18.8%)\n",
            "  6 turns: 184 conversations (5.3%)\n",
            "  7 turns: 52 conversations (1.5%)\n",
            "  8 turns: 16 conversations (0.5%)\n",
            "  9 turns: 2 conversations (0.1%)\n"
          ]
        }
      ],
      "source": [
        "# Analyze conversation patterns\n",
        "conv_lengths = []\n",
        "simple_conversations = 0\n",
        "hybrid_conversations = 0\n",
        "\n",
        "for conversations in dataset.values():\n",
        "    for conv in conversations:\n",
        "        dialogue = conv['dialogue']\n",
        "        num_turns = len(dialogue['conv_questions'])\n",
        "        conv_lengths.append(num_turns)\n",
        "        \n",
        "        # Check conversation type\n",
        "        qa_split = dialogue.get('qa_split', [])\n",
        "        if qa_split and any(qa_split):\n",
        "            hybrid_conversations += 1\n",
        "        else:\n",
        "            simple_conversations += 1\n",
        "\n",
        "print(\"Conversation length statistics:\")\n",
        "print(f\"  Average: {np.mean(conv_lengths):.2f} turns\")\n",
        "print(f\"  Median: {np.median(conv_lengths):.0f} turns\")\n",
        "print(f\"  Min: {min(conv_lengths)} turns\")\n",
        "print(f\"  Max: {max(conv_lengths)} turns\")\n",
        "\n",
        "print(f\"\\nConversation types:\")\n",
        "total = simple_conversations + hybrid_conversations\n",
        "print(f\"  Simple (Type I): {simple_conversations} ({simple_conversations/total*100:.1f}%)\")\n",
        "print(f\"  Hybrid (Type II): {hybrid_conversations} ({hybrid_conversations/total*100:.1f}%)\")\n",
        "\n",
        "# Length distribution\n",
        "length_counts = Counter(conv_lengths)\n",
        "print(f\"\\nLength distribution:\")\n",
        "for length in sorted(length_counts.keys()):\n",
        "    count = length_counts[length]\n",
        "    pct = count / len(conv_lengths) * 100\n",
        "    print(f\"  {length} turns: {count} conversations ({pct:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample conversation from train split:\n",
            "ID: Single_JKHY/2009/page_28.pdf-3\n",
            "\n",
            "Document context:\n",
            "Pre-text: 26 | 2009 annual report in fiscal 2008 , revenues in the credit union systems and services business ...\n",
            "Post-text: year ended june 30 , cash provided by operations increased $ 25587 to $ 206588 for the fiscal year e...\n",
            "\n",
            "Table structure:\n",
            "Columns: 3\n",
            "Column names: ['Year ended June 30, 2009', '2008', '2007']...\n",
            "Rows: 6\n",
            "\n",
            "Conversation (4 turns):\n",
            "\n",
            "Turn 1:\n",
            "  Q: what is the net cash from operating activities in 2009?\n",
            "  A: 206588\n",
            "  Program: 206588\n",
            "\n",
            "Turn 2:\n",
            "  Q: what about in 2008?\n",
            "  A: 181001\n",
            "  Program: 181001\n",
            "\n",
            "  ... 2 more turns\n"
          ]
        }
      ],
      "source": [
        "# Look at a sample conversation\n",
        "first_split = list(dataset.keys())[0]\n",
        "sample_conv = dataset[first_split][0]\n",
        "\n",
        "print(f\"Sample conversation from {first_split} split:\")\n",
        "print(f\"ID: {sample_conv['id']}\")\n",
        "\n",
        "# Document context\n",
        "doc = sample_conv['doc']\n",
        "print(f\"\\nDocument context:\")\n",
        "print(f\"Pre-text: {doc['pre_text'][:100]}...\")\n",
        "print(f\"Post-text: {doc['post_text'][:100]}...\")\n",
        "\n",
        "# Table info\n",
        "table = doc['table']\n",
        "print(f\"\\nTable structure:\")\n",
        "print(f\"Columns: {len(table)}\")\n",
        "print(f\"Column names: {list(table.keys())[:3]}...\")\n",
        "\n",
        "if table:\n",
        "    first_col = list(table.keys())[0]\n",
        "    print(f\"Rows: {len(table[first_col])}\")\n",
        "\n",
        "# Show conversation\n",
        "dialogue = sample_conv['dialogue']\n",
        "print(f\"\\nConversation ({len(dialogue['conv_questions'])} turns):\")\n",
        "\n",
        "for i, (q, a, p) in enumerate(zip(\n",
        "    dialogue['conv_questions'][:2],\n",
        "    dialogue['conv_answers'][:2], \n",
        "    dialogue['turn_program'][:2]\n",
        ")):\n",
        "    print(f\"\\nTurn {i+1}:\")\n",
        "    print(f\"  Q: {q}\")\n",
        "    print(f\"  A: {a}\")\n",
        "    print(f\"  Program: {p}\")\n",
        "\n",
        "if len(dialogue['conv_questions']) > 2:\n",
        "    remaining = len(dialogue['conv_questions']) - 2\n",
        "    print(f\"\\n  ... {remaining} more turns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TRAIN split record IDs (first 5):\n",
            "  Single_JKHY/2009/page_28.pdf-3\n",
            "  Single_RSG/2008/page_114.pdf-2\n",
            "  Single_AAPL/2002/page_23.pdf-1\n",
            "  Single_UPS/2009/page_33.pdf-2\n",
            "  Double_UPS/2009/page_33.pdf\n",
            "\n",
            "DEV split record IDs (first 5):\n",
            "  Single_MRO/2007/page_134.pdf-1\n",
            "  Double_HII/2017/page_104.pdf\n",
            "  Single_IPG/2009/page_85.pdf-3\n",
            "  Single_UNP/2008/page_77.pdf-2\n",
            "  Double_RSG/2016/page_144.pdf\n"
          ]
        }
      ],
      "source": [
        "# Print first few record IDs from each split to understand the format\n",
        "for split_name, conversations in dataset.items():\n",
        "    print(f\"\\n{split_name.upper()} split record IDs (first 5):\")\n",
        "    for conv in conversations[:5]:\n",
        "        print(f\"  {conv['id']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Searching for JKHY records:\n",
            "\n",
            "TRAIN split JKHY records:\n",
            "  Single_JKHY/2009/page_28.pdf-3\n",
            "  Double_JKHY/2016/page_25.pdf\n",
            "  Double_JKHY/2015/page_20.pdf\n",
            "  Double_JKHY/2016/page_61.pdf\n",
            "  Single_JKHY/2008/page_30.pdf-1\n",
            "  Double_JKHY/2008/page_30.pdf\n",
            "  Single_JKHY/2009/page_28.pdf-4\n",
            "\n",
            "DEV split JKHY records:\n",
            "  Single_JKHY/2014/page_30.pdf-2\n"
          ]
        }
      ],
      "source": [
        "# Search for JKHY records\n",
        "print(\"\\nSearching for JKHY records:\")\n",
        "for split_name, conversations in dataset.items():\n",
        "    jkhy_records = [conv['id'] for conv in conversations if 'JKHY' in conv['id']]\n",
        "    if jkhy_records:\n",
        "        print(f\"\\n{split_name.upper()} split JKHY records:\")\n",
        "        for record_id in jkhy_records:\n",
        "            print(f\"  {record_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample valid record IDs you can use:\n",
            "\n",
            "TRAIN split examples:\n",
            "ID: Single_GS/2018/page_134.pdf-3\n",
            "First question: what was the value of total financial assets in 2018?\n",
            "\n",
            "ID: Single_LMT/2014/page_31.pdf-2\n",
            "First question: what was the average price of the purchased shares in november 2014?\n",
            "\n",
            "ID: Single_ETR/2004/page_163.pdf-5\n",
            "First question: what is the difference in receivables from the money pool between 2001 and 2022?\n",
            "\n",
            "ID: Single_ADI/2016/page_61.pdf-2\n",
            "First question: what was the balance of goodwill at the end of 2016?\n",
            "\n",
            "ID: Double_HWM/2015/page_87.pdf\n",
            "First question: in the year of 2013, what were the intersegment sales as a percentage of the total sales?\n",
            "\n",
            "\n",
            "DEV split examples:\n",
            "ID: Single_SNA/2012/page_33.pdf-1\n",
            "First question: what is the value of the investment in snap-onincorporated\tin 2008?\n",
            "\n",
            "ID: Single_AAP/2011/page_28.pdf-2\n",
            "First question: what is the value of the s&p 500 index on january 3, 2009 less it at the end of 2006?\n",
            "\n",
            "ID: Double_GPN/2017/page_77.pdf\n",
            "First question: what portion of the total identifiable net assets in cash?\n",
            "\n",
            "ID: Double_ABMD/2009/page_88.pdf\n",
            "First question: what was the total of contingent payments related to impella?\n",
            "\n",
            "ID: Single_APTV/2014/page_49.pdf-2\n",
            "First question: what is the net change in value of delphi automotive plc from 2011 to 2014?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print 5 random record IDs from each split\n",
        "import random\n",
        "\n",
        "print(\"Sample valid record IDs you can use:\")\n",
        "for split_name, conversations in dataset.items():\n",
        "    sample_records = random.sample(conversations, min(5, len(conversations)))\n",
        "    print(f\"\\n{split_name.upper()} split examples:\")\n",
        "    for record in sample_records:\n",
        "        # Also print first question to understand the context\n",
        "        first_question = record['dialogue']['conv_questions'][0]\n",
        "        print(f\"ID: {record['id']}\")\n",
        "        print(f\"First question: {first_question}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Searching for JKHY records with cash flow information:\n",
            "\n",
            "TRAIN split relevant records:\n",
            "\n",
            "ID: Single_JKHY/2009/page_28.pdf-3\n",
            "First question: what is the net cash from operating activities in 2009?\n",
            "\n",
            "ID: Single_RSG/2012/page_145.pdf-1\n",
            "First question: what was the total restricted cash and marketable securities in 2012?\n",
            "\n",
            "ID: Single_WRK/2019/page_49.pdf-1\n",
            "First question: what was the sum of net cash provided by operating activities and used for investing in 2019?\n",
            "\n",
            "ID: Double_WRK/2018/page_53.pdf\n",
            "First question: in the year of 2018, what was the total net cash used?\n",
            "\n",
            "ID: Single_SWKS/2010/page_105.pdf-2\n",
            "First question: what is the balance of cash and cash equivalents at the end of 2010?\n",
            "\n",
            "DEV split relevant records:\n",
            "\n",
            "ID: Single_UNP/2014/page_35.pdf-3\n",
            "First question: what was the cash provided by operating activities in 2013?\n",
            "\n",
            "ID: Single_IPG/2008/page_93.pdf-3\n",
            "First question: what is the total of estimated future contingent acquisition obligations payable in cash in 2009?\n",
            "\n",
            "ID: Single_LMT/2010/page_42.pdf-3\n",
            "First question: what is the net cash from operating and investing activities?\n",
            "\n",
            "ID: Single_BLL/2007/page_35.pdf-3\n",
            "First question: what was the total amount of cash outflow used for shares repurchased during november 2007, in millions of dollars?\n",
            "\n",
            "ID: Double_RSG/2009/page_100.pdf\n",
            "First question: during 2009, what was the total of additions charged to expense?\n"
          ]
        }
      ],
      "source": [
        "# Search specifically for records related to JKHY and cash flow\n",
        "print(\"\\nSearching for JKHY records with cash flow information:\")\n",
        "for split_name, conversations in dataset.items():\n",
        "    relevant_records = [\n",
        "        conv for conv in conversations \n",
        "        if ('JKHY' in conv['id'] or 'cash' in ' '.join(conv['dialogue']['conv_questions']).lower())\n",
        "    ]\n",
        "    if relevant_records:\n",
        "        print(f\"\\n{split_name.upper()} split relevant records:\")\n",
        "        for record in relevant_records[:5]:  # Show first 5 matches\n",
        "            print(f\"\\nID: {record['id']}\")\n",
        "            print(f\"First question: {record['dialogue']['conv_questions'][0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Document Analysis\n",
        "\n",
        "Let's analyze all unique documents in the dataset and their characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique documents: 2462\n",
            "\n",
            "Document statistics:\n",
            "       conversations  table_columns  table_rows  pre_text_length  \\\n",
            "count         2462.0        2462.00     2462.00          2462.00   \n",
            "mean             1.4           2.82        5.28          1730.94   \n",
            "std              0.7           1.53        2.55          1383.42   \n",
            "min              1.0           1.00        1.00             1.00   \n",
            "25%              1.0           2.00        3.00           523.25   \n",
            "50%              1.0           3.00        5.00          1397.50   \n",
            "75%              2.0           3.00        7.00          2741.00   \n",
            "max              6.0          10.00       19.00          7153.00   \n",
            "\n",
            "       post_text_length  total_text_length  \n",
            "count           2462.00            2462.00  \n",
            "mean            1907.50            3638.45  \n",
            "std             1495.59            1480.93  \n",
            "min                1.00             103.00  \n",
            "25%              649.50            2854.00  \n",
            "50%             1743.00            3627.50  \n",
            "75%             2974.50            4412.00  \n",
            "max             8769.00           14166.00  \n",
            "\n",
            "Split distribution:\n",
            "  train: 2164 documents\n",
            "  dev: 298 documents\n",
            "\n",
            "Top 5 documents by number of conversations:\n",
            "                      document_id  conversations splits  table_columns  \\\n",
            "779   Single_RE/2010/page_138.pdf              6  train              3   \n",
            "3     Single_UPS/2009/page_33.pdf              4  train              6   \n",
            "18   Single_HII/2015/page_120.pdf              4  train              4   \n",
            "21    Single_STT/2013/page_54.pdf              4  train              6   \n",
            "46    Single_RCL/2006/page_37.pdf              4  train              2   \n",
            "\n",
            "     table_rows  \n",
            "779           7  \n",
            "3             3  \n",
            "18            7  \n",
            "21            4  \n",
            "46            5  \n"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to store unique documents\n",
        "unique_docs = {}\n",
        "\n",
        "# Iterate through all splits and conversations\n",
        "for split_name, conversations in dataset.items():\n",
        "    for conv in conversations:\n",
        "        doc_id = conv['id'].split('-')[0]  # Get base document ID without conversation number\n",
        "        \n",
        "        if doc_id not in unique_docs:\n",
        "            unique_docs[doc_id] = {\n",
        "                'doc': conv['doc'],\n",
        "                'conversation_count': 1,\n",
        "                'splits': {split_name},\n",
        "                'table_columns': len(conv['doc']['table']),\n",
        "                'table_rows': len(next(iter(conv['doc']['table'].values()))),\n",
        "                'pre_text_length': len(conv['doc']['pre_text']),\n",
        "                'post_text_length': len(conv['doc']['post_text'])\n",
        "            }\n",
        "        else:\n",
        "            unique_docs[doc_id]['conversation_count'] += 1\n",
        "            unique_docs[doc_id]['splits'].add(split_name)\n",
        "\n",
        "# Convert to DataFrame for better analysis\n",
        "docs_df = pd.DataFrame([\n",
        "    {\n",
        "        'document_id': doc_id,\n",
        "        'conversations': info['conversation_count'],\n",
        "        'splits': ', '.join(sorted(info['splits'])),\n",
        "        'table_columns': info['table_columns'],\n",
        "        'table_rows': info['table_rows'],\n",
        "        'pre_text_length': info['pre_text_length'],\n",
        "        'post_text_length': info['post_text_length'],\n",
        "        'total_text_length': info['pre_text_length'] + info['post_text_length']\n",
        "    }\n",
        "    for doc_id, info in unique_docs.items()\n",
        "])\n",
        "\n",
        "# Display summary statistics\n",
        "print(f\"Total unique documents: {len(docs_df)}\")\n",
        "print(\"\\nDocument statistics:\")\n",
        "print(docs_df.describe().round(2))\n",
        "\n",
        "print(\"\\nSplit distribution:\")\n",
        "split_counts = docs_df['splits'].value_counts()\n",
        "for split_type, count in split_counts.items():\n",
        "    print(f\"  {split_type}: {count} documents\")\n",
        "\n",
        "# Display first few documents with most conversations\n",
        "print(\"\\nTop 5 documents by number of conversations:\")\n",
        "print(docs_df.nlargest(5, 'conversations')[['document_id', 'conversations', 'splits', 'table_columns', 'table_rows']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       Single_JKHY/2009/page_28.pdf\n",
              "1       Single_RSG/2008/page_114.pdf\n",
              "2       Single_AAPL/2002/page_23.pdf\n",
              "3        Single_UPS/2009/page_33.pdf\n",
              "4        Double_UPS/2009/page_33.pdf\n",
              "                    ...             \n",
              "2457    Single_VRTX/2003/page_71.pdf\n",
              "2458     Single_MRO/2006/page_33.pdf\n",
              "2459    Double_ADBE/2018/page_86.pdf\n",
              "2460     Double_DVN/2014/page_85.pdf\n",
              "2461    Single_JKHY/2014/page_30.pdf\n",
              "Name: document_id, Length: 2462, dtype: object"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_df['document_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Details of the document with most conversations:\n",
            "Document ID: Single_RE/2010/page_138.pdf\n",
            "Number of conversations: 6\n",
            "Present in splits: train\n",
            "\n",
            "Table Structure:\n",
            "Columns (3): ['2010', '2009', '2008']\n",
            "Rows: 7\n",
            "\n",
            "Pre-text:\n",
            "a reconciliation of the beginning and ending amount of unrecognized tax benefits , for the periods indicated , is as follows: .\n",
            "\n",
            "Post-text:\n",
            "the entire amount of the unrecognized tax benefits would affect the effective tax rate if recognized . in 2010 , the company favorably settled a 2003 and 2004 irs audit . the company recorded a net ov...\n"
          ]
        }
      ],
      "source": [
        "# Function to display document details\n",
        "def display_document_details(doc_id):\n",
        "    doc_info = unique_docs[doc_id]\n",
        "    doc = doc_info['doc']\n",
        "    \n",
        "    print(f\"Document ID: {doc_id}\")\n",
        "    print(f\"Number of conversations: {doc_info['conversation_count']}\")\n",
        "    print(f\"Present in splits: {', '.join(sorted(doc_info['splits']))}\")\n",
        "    \n",
        "    print(\"\\nTable Structure:\")\n",
        "    print(f\"Columns ({len(doc['table'])}): {list(doc['table'].keys())}\")\n",
        "    print(f\"Rows: {len(next(iter(doc['table'].values())))}\")\n",
        "    \n",
        "    print(\"\\nPre-text:\")\n",
        "    print(doc['pre_text'][:200] + \"...\" if len(doc['pre_text']) > 200 else doc['pre_text'])\n",
        "    \n",
        "    print(\"\\nPost-text:\")\n",
        "    print(doc['post_text'][:200] + \"...\" if len(doc['post_text']) > 200 else doc['post_text'])\n",
        "\n",
        "# Display details of the document with the most conversations\n",
        "most_conv_doc = docs_df.nlargest(1, 'conversations').iloc[0]['document_id']\n",
        "print(\"Details of the document with most conversations:\")\n",
        "display_document_details(most_conv_doc)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
